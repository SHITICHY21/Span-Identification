{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas pyarrow\n!pip install transformers datasets torch pandas numpy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.idle":"2025-03-26T10:29:01.296058Z","shell.execute_reply.started":"2025-03-26T10:28:53.973887Z","shell.execute_reply":"2025-03-26T10:29:01.295218Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets torch pandas numpy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:29:01.297330Z","iopub.execute_input":"2025-03-26T10:29:01.297561Z","iopub.status.idle":"2025-03-26T10:29:04.705003Z","shell.execute_reply.started":"2025-03-26T10:29:01.297538Z","shell.execute_reply":"2025-03-26T10:29:04.703965Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the data (adjust paths based on your environment)\ntrain_df = pd.read_parquet('/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet')\ntest_df = pd.read_csv('/kaggle/input/unlp-2025-shared-task-span-identification/test.csv')\n\n# Check the data\nprint(\"TRAIN DATA\",train_df.head(2))\nprint(\"TEST DATA\", test_df.head(2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:29:04.706070Z","iopub.execute_input":"2025-03-26T10:29:04.706345Z","iopub.status.idle":"2025-03-26T10:29:05.224434Z","shell.execute_reply.started":"2025-03-26T10:29:04.706319Z","shell.execute_reply":"2025-03-26T10:29:05.223688Z"}},"outputs":[{"name":"stdout","text":"TRAIN DATA                                      id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n\n                                             content lang  manipulative  \\\n0  Новий огляд мапи DeepState від російського вій...   uk          True   \n1  Недавно 95 квартал жёстко поглумился над русск...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n\n                                   trigger_words  \n0    [[27, 63], [65, 88], [90, 183], [186, 308]]  \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]  \nTEST DATA                                      id  \\\n0  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba   \n1  9b2a61e4-d14e-4ff7-b304-e73d720319bf   \n\n                                             content  \n0  Они просрали нашу технику, положили кучу людей...  \n1  ❗️\\nКитай предлагает отдать оккупированные тер...  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install spacy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:29:05.225243Z","iopub.execute_input":"2025-03-26T10:29:05.225559Z","iopub.status.idle":"2025-03-26T10:29:08.672650Z","shell.execute_reply.started":"2025-03-26T10:29:05.225527Z","shell.execute_reply":"2025-03-26T10:29:08.671574Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.11.0a2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport spacy\n\n# Load spaCy for word tokenization (generic multi-language tokenizer)\nnlp = spacy.blank(\"xx\")  # 'xx' is the generic tokenizer spaCy provides\n\n# Function to clean text\ndef clean_text(text):\n    # Lowercasing\n    text = text.lower()\n    # Remove URLs, mentions, hashtags, and special characters\n    text = re.sub(r'http\\S+|www\\S+|#\\S+|@\\S+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n    # Remove extra whitespaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Tokenizer function using spaCy\ndef tokenize_words(text):\n    doc = nlp(text)  # Tokenize the text using spaCy's default tokenizer\n    return [token.text for token in doc]\n\n# Optimized function to parse spans directly from the list of lists (trigger_words)\ndef parse_spans(span_str):\n    # Ensure the input is a string or handle other cases\n    if isinstance(span_str, str):  # If it is a string\n        if pd.isna(span_str) or span_str.strip() == \"\":  # Handle empty or NaN values\n            return []\n        try:\n            # Extract all numbers from the string\n            numbers = list(map(int, re.findall(r'\\d+', span_str)))\n            # Convert list into tuples of (start, end) spans\n            if len(numbers) % 2 != 0:\n                # If the number of extracted numbers is odd, it means one of the spans is incomplete\n                return []\n            spans = [(numbers[i], numbers[i + 1]) for i in range(0, len(numbers), 2)]\n            return spans\n        except Exception as e:\n            # Log parsing errors for debugging\n            print(f\"Parsing error: {e} in {span_str}\")\n            return []\n    elif isinstance(span_str, np.ndarray):  # If it is a numpy array, convert it to string and parse again\n        span_str = ' '.join(str(x) for x in span_str)\n        return parse_spans(span_str)  # Recurse with the converted string\n    elif span_str is None:  # Handle NoneType\n        return []\n    else:\n        # Handle cases where the input is not a string or numpy array\n        print(f\"Unexpected type: {type(span_str)}. Skipping this entry.\")\n        return []\n\n# Load the data (adjust paths based on your environment)\ntrain_df = pd.read_parquet('/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet')\ntest_df = pd.read_csv('/kaggle/input/unlp-2025-shared-task-span-identification/test.csv')\n\n# Apply text cleaning to both train and test data\ntrain_df['cleaned_content'] = train_df['content'].apply(clean_text)\ntest_df['cleaned_content'] = test_df['content'].apply(clean_text)\n\n# Tokenize train and test data by words\ntrain_df['tokens'] = train_df['cleaned_content'].apply(tokenize_words)\ntest_df['tokens'] = test_df['cleaned_content'].apply(tokenize_words)\n\n# Apply the function to the 'trigger_words' column of the train dataset to parse spans\ntrain_df[\"parsed_spans\"] = train_df[\"trigger_words\"].apply(parse_spans)\n\n# Display the first few rows to ensure correct processing\nprint(train_df[[\"trigger_words\", \"parsed_spans\"]].head())\n\n# Optionally, you can check the cleaned content and tokens as well\nprint(\"Sample cleaned content and tokens:\")\nprint(train_df[['cleaned_content', 'tokens']].head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:29:08.674953Z","iopub.execute_input":"2025-03-26T10:29:08.675269Z","iopub.status.idle":"2025-03-26T10:29:18.838007Z","shell.execute_reply.started":"2025-03-26T10:29:08.675241Z","shell.execute_reply":"2025-03-26T10:29:18.837156Z"}},"outputs":[{"name":"stdout","text":"                                   trigger_words  \\\n0    [[27, 63], [65, 88], [90, 183], [186, 308]]   \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]   \n2                                    [[55, 100]]   \n3                                           None   \n4                                   [[114, 144]]   \n\n                                    parsed_spans  \n0    [(27, 63), (65, 88), (90, 183), (186, 308)]  \n1  [(0, 40), (123, 137), (180, 251), (253, 274)]  \n2                                    [(55, 100)]  \n3                                             []  \n4                                   [(114, 144)]  \nSample cleaned content and tokens:\n                                     cleaned_content  \\\n0  новий огляд мапи deepstate від російського вій...   \n1  недавно 95 квартал жёстко поглумился над русск...   \n2  тим часом йде евакуація бєлгородського автовок...   \n3  в україні найближчим часом мають намір посилит...   \n4  расчёты 122мм сау 2с1 гвоздика 132й бригады 1г...   \n\n                                              tokens  \n0  [новий, огляд, мапи, deepstate, від, російсько...  \n1  [недавно, 95, квартал, жёстко, поглумился, над...  \n2  [тим, часом, йде, евакуація, бєлгородського, а...  \n3  [в, україні, найближчим, часом, мають, намір, ...  \n4  [расчёты, 122, мм, сау, 2с1, гвоздика, 132й, б...  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n# import pandas as pd\n# import numpy as np\n# import re\n\n# # Load the XLM-RoBERTa tokenizer\n# model_name = \"bert-base-multilingual-cased\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# # Function to convert text and spans into tokens and labels\n# def create_bio_labels(text, spans):\n#     # Tokenize the text by splitting into words (not using tokenizer here, just word split)\n#     tokens = text.split()  # Split by whitespace\n#     labels = [\"O\"] * len(tokens)  # Initialize all labels as \"O\"\n    \n#     # Convert the start and end spans into token-based indices\n#     for start_idx, end_idx in spans:\n#         found = False\n#         for i, token in enumerate(tokens):\n#             token_start = sum(len(tokens[j]) + 1 for j in range(i))  # Count the length of the words before it\n#             token_end = token_start + len(token) - 1\n\n#             # Check if the token is within the span range\n#             if token_start >= start_idx and token_end <= end_idx:\n#                 if not found:  # First token of the span\n#                     labels[i] = \"B-MANIPULATIVE\"\n#                     found = True\n#                 else:  # Inside the span\n#                     labels[i] = \"I-MANIPULATIVE\"\n\n#     # Ensure that the number of tokens matches the length of labels\n#     assert len(tokens) == len(labels), f\"Mismatch in tokens and labels lengths: {len(tokens)} vs {len(labels)}\"\n    \n#     return tokens, labels\n\n# # Example: Apply BIO tagging to the first few examples\n# for i in range(2):\n#     text = train_df.iloc[i][\"cleaned_content\"]  # Use the cleaned content\n#     spans = train_df.iloc[i][\"parsed_spans\"]  # Use the parsed spans\n#     tokens, labels = create_bio_labels(text, spans)\n\n#     print(f\"Example {i+1}:\")\n#     print(\"Tokens:\", tokens)\n#     print(\"Labels:\", labels)\n#     print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:29:18.839587Z","iopub.execute_input":"2025-03-26T10:29:18.839903Z","iopub.status.idle":"2025-03-26T10:29:18.844042Z","shell.execute_reply.started":"2025-03-26T10:29:18.839873Z","shell.execute_reply":"2025-03-26T10:29:18.843278Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\nimport numpy as np\nimport re\n\n# Load the XLM-RoBERTa tokenizer\nmodel_name = \"bert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Function to convert text and spans into tokens and labels\ndef create_bio_labels(text, spans):\n    # Tokenize the text\n    encoding = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_offsets_mapping=True)\n    \n    # Convert input_ids to tokens (tokens are not directly available, so we convert from IDs)\n    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n    token_offsets = encoding['offset_mapping']\n    \n    # Initialize all labels as \"O\"\n    labels = [\"O\"] * len(tokens)\n    \n    for start_idx, end_idx in spans:\n        found = False\n        for i, (token_start, token_end) in enumerate(token_offsets):\n            # Check if the token is within the span range\n            if token_start >= start_idx and token_end <= end_idx:\n                if not found:  # First token of the span\n                    labels[i] = \"B-MANIPULATIVE\"\n                    found = True\n                else:  # Inside the span\n                    labels[i] = \"I-MANIPULATIVE\"\n    \n    # Ensure that the number of tokens matches the length of labels\n    assert len(tokens) == len(labels), f\"Mismatch in tokens and labels lengths: {len(tokens)} vs {len(labels)}\"\n    \n    return tokens, labels\n\n# Example: Apply BIO tagging to the first few examples\nfor i in range(2):\n    text = train_df.iloc[i][\"cleaned_content\"]  # Use the cleaned content\n    spans = train_df.iloc[i][\"parsed_spans\"]  # Use the parsed spans\n    tokens, labels = create_bio_labels(text, spans)\n\n    print(f\"Example {i+1}:\")\n    print(\"Tokens:\", tokens)\n    print(\"Labels:\", labels)\n    print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:29:18.844881Z","iopub.execute_input":"2025-03-26T10:29:18.845189Z","iopub.status.idle":"2025-03-26T10:29:20.941943Z","shell.execute_reply.started":"2025-03-26T10:29:18.845158Z","shell.execute_reply":"2025-03-26T10:29:20.941216Z"}},"outputs":[{"name":"stdout","text":"Example 1:\nTokens: ['[CLS]', 'новий', 'о', '##гляд', 'ма', '##пи', 'deep', '##state', 'від', 'рос', '##ійського', 'військового', 'е', '##кс', '##пер', '##та', 'к', '##уха', '##ра', 'пут', '##іна', '2', 'р', '##оз', '##ряду', 'с', '##пе', '##ці', '##алі', '##ста', 'по', 'с', '##нар', '##яд', '##ному', 'голо', '##ду', 'та', 'рек', '##тора', 'му', '##зи', '##чної', 'академії', 'м', '##ін', '##об', '##оро', '##ни', 'р', '##ф', 'є', '##в', '##г', '##є', '##нія', 'при', '##го', '##жина', 'при', '##го', '##жин', 'про', '##г', '##но', '##зу', '##є', 'що', 'не', '##в', '##дов', '##зі', 'нас', '##тан', '##е', 'день', 'зв', '##іль', '##нення', 'к', '##рим', '##у', 'і', 'день', 'р', '##оз', '##паду', 'рос', '##ії', 'каже', 'що', 'перед', '##ум', '##ови', 'цього', 'вже', 'ст', '##вор', '##ені', 'від', '##ео', 'взяли', 'з', 'канал', '##у', 'ф', '##д', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'O', 'B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'O', 'B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n--------------------------------------------------------------------------------\nExample 2:\nTokens: ['[CLS]', 'не', '##давно', '95', 'кв', '##арта', '##л', 'ж', '##ёс', '##тко', 'по', '##г', '##лу', '##ми', '##лся', 'над', 'рус', '##скими', 'бара', '##нами', 'которые', 'при', '##твор', '##яются', 'укр', '##аи', '##нца', '##ми', 'про', 'с', '##кад', '##ов', '##ск', 'там', 'ш', '##уто', '##чки', 'были', 'деген', '##ера', '##тив', '##ные', 'и', 'что', 'пред', '##лага', '##ют', 'сделать', 'русские', 'бара', '##ны', 'как', 'обычно', 'стать', 'на', 'кол', '##ени', 'и', 'ли', '##за', '##ть', 'укр', '##аи', '##нскую', 'ту', '##ф', '##лю', 'чтобы', 'их', 'про', '##сти', '##ли', 'укр', '##аи', '##нцы', 'же', 'как', 'не', '##гр', '##ы', 'перед', 'ними', 'нужно', 'б', '##ес', '##кон', '##ечно', 'из', '##вин', '##яться', 'м', '##эр', 'ок', '##куп', '##ирован', '##ного', 'с', '##кад', '##ов', '##ска', 'як', '##ов', '##лев', 'про', '##ком', '##менти', '##ровал', 'номер', 'кв', '##арта', '##ла', '95', 'над', '##ею', '##сь', 'у', 'реж', '##ис', '##сер', '##ов', 'х', '##вати', '##т', 'сил', 'из', '##вин', '##иться', 'так', 'глава', 'города', 'приз', '##вал', 'реж', '##ис', '##сер', '##ов', 'шоу', 'из', '##вин', '##иться', 'перед', 'людьми', 'за', 'ш', '##ут', '##ки', 'очень', 'прав', 'был', 'пет', '##лю', '##ра', 'который', 'говори', '##л', 'нам', 'не', 'так', 'стр', '##аш', '##ны', 'м', '##ос', '##ков', '##ские', 'в', '##ши', 'нам', 'стр', '##аш', '##ны', 'укр', '##аи', '##нские', 'г', '##ни', '##ды', 'ш', '##ут', '##ки', 'о', 'се', '##ська', '##х', 'из', 'с', '##кад', '##ов', '##ска', 'на', '##вер', '##ное', 'ко', '##му', '##то', 'ка', '##жу', '##тся', 'очень', 'ос', '##тро', '##ум', '##ными', 'но', 'п', '##ус', '##ть', 'это', 'будет', 'на', 'сов', '##ести', 'реж', '##ис', '##сер', '##ов', 'над', '##ею', '##сь', 'у', 'них', 'х', '##вати', '##т', 'силы', 'из', '##вин', '##иться', 'перед', 'укр', '##аи', '##нца', '##ми', 'с', '##кад', '##ов', '##ск', 'сегодня', 'в', 'ок', '##куп', '##ации', 'и', 'дей', '##ств', '##итель', '##но', 'чего', 'сейчас', 'н', '##у', '##жда', '##ются', 'с', '##кад', '##ов', '##чан', '##е', 'в', 'поддержке', 'з', '##на', '##ю', 'что', 'у', 'нас', 'много', 'людей', 'были', 'рус', '##ско', '##яз', '##ы', '##чными', 'но', 'все', 'хорошо', 'по', '##нимают', 'украинский', 'и', 'хорошо', 'говорят', 'по', '##ук', '##раи', '##нски', 'от', '##метил', 'м', '##эр', 'як', '##ов', '##лев', 'также', 'объявил', 'ф', '##ле', '##ш', '##мо', '##б', 'с', '##кад', '##ов', '##ск', 'говорит', 'по', '##ук', '##раи', '##нски', 'он', 'предложил', 'всем', 'ж', '##ителя', '##м', 'с', '##кад', '##ов', '##ска', 'за', '##читать', 'что', '##ни', '##будь', 'на', 'укр', '##аи', '##нском', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\nLabels: ['B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'O', 'B-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE', 'I-MANIPULATIVE']\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:29:20.942878Z","iopub.execute_input":"2025-03-26T10:29:20.943482Z","iopub.status.idle":"2025-03-26T10:29:20.946920Z","shell.execute_reply.started":"2025-03-26T10:29:20.943443Z","shell.execute_reply":"2025-03-26T10:29:20.946139Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import time\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nfrom transformers import EarlyStoppingCallback\nimport torch.nn as nn\nfrom transformers import DataCollatorForTokenClassification\n\n# ✅ Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}\")\n\n# Load XLM-Roberta Tokenizer\nmodel_name = \"xlm-roberta-large\"  # Replacing XLM-RoBERTa with BERT Multilingual\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Label mapping\nlabel2id = {\"O\": 0, \"B-MANIPULATIVE\": 1, \"I-MANIPULATIVE\": 2}\nid2label = {v: k for k, v in label2id.items()}\n\n# Function to encode BIO labels into numerical format\ndef encode_labels(labels):\n    return [label2id[label] for label in labels]\n\n# Process Train Data (you should have your train_df loaded here)\n# Process Train Data\ntrain_texts, train_labels_encoded = [], []\nfor _, row in train_df.iterrows():\n    text = row[\"content\"]\n    spans = row[\"parsed_spans\"]\n    tokens, labels = create_bio_labels(text, spans)  # This will return both tokens and labels\n    train_texts.append(text)\n    train_labels_encoded.append(encode_labels(labels))\n\n\n# ✅ Split dataset: 80% train, 20% validation\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_texts, train_labels_encoded, test_size=0.2, random_state=42\n)\n\n# PyTorch Dataset Class\nclass SocialMediaDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(self.texts[idx], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")  # Reduced max_length\n        label_ids = [-100] * len(encoding[\"input_ids\"][0])  # Ignore padding tokens\n\n        for i, label in enumerate(self.labels[idx][:len(label_ids)]):\n            label_ids[i] = label\n\n        encoding[\"labels\"] = torch.tensor(label_ids)\n        \n        # Move tensors to the correct device\n        return {key: val.squeeze(0).to(device) for key, val in encoding.items()}\n\n# ✅ Create DataLoaders with reduced batch size\ntrain_dataset = SocialMediaDataset(train_texts, train_labels)\nval_dataset = SocialMediaDataset(val_texts, val_labels)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size to fit memory\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Reduced batch size for validation\n\n# BiLSTM-XLMRoBerta model\nclass BiLSTM_XLMRoBerta(nn.Module):\n    def __init__(self, model_name, num_labels, hidden_size=1024, lstm_hidden_size=256, num_layers=2, dropout=0.1):\n        super(BiLSTM_XLMRoBerta, self).__init__()\n        \n        # Load the pre-trained XLM-Roberta model\n        self.transformer = AutoModelForTokenClassification.from_pretrained(\n            model_name, \n            num_labels=num_labels\n        ).base_model  # Extract base model without the classification head\n        \n        # BiLSTM Layer\n        self.bilstm = nn.LSTM(\n            input_size=hidden_size,  # Set to 1024 as XLM-Roberta's output size\n            hidden_size=lstm_hidden_size, \n            num_layers=num_layers, \n            bidirectional=True, \n            batch_first=True, \n            dropout=dropout\n        )\n        \n        # Linear layer for token classification\n        self.classifier = nn.Linear(lstm_hidden_size * 2, num_labels)  # *2 because it's bidirectional\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        # Transformer output\n        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = transformer_output.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n        \n        # BiLSTM output\n        lstm_output, _ = self.bilstm(sequence_output)\n        \n        # Classifier output (logits)\n        logits = self.classifier(lstm_output)  # Shape: (batch_size, seq_len, num_labels)\n\n        # If labels are provided, calculate loss\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n            # Flatten the tokens and labels for loss computation\n            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n\n        return (loss, logits) if loss is not None else logits\n\n\n# ✅ Load the new custom model\nmodel = BiLSTM_XLMRoBerta(\n    model_name=\"xlm-roberta-large\", \n    num_labels=len(label2id),\n    hidden_size=1024,  # XLM-Roberta output hidden size\n    lstm_hidden_size=256,  # LSTM hidden size, you can tune this\n    num_layers=2,  # Number of LSTM layers\n    dropout=0.1  # Dropout for regularization\n).to(device)\n\n# ✅ Define Optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# ✅ Training Arguments with gradient accumulation and FP16\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",  # ✅ Evaluates at the end of each epoch\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,  # Reduced batch size\n    num_train_epochs=27,  # More epochs\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=4,  # Gradient Accumulation for larger batch simulation\n    fp16=True,  # Mixed precision for memory efficiency\n)\n\n# ✅ Early stopping callback\nearly_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)  # Stops after 2 epochs of no improvement\n\n# ✅ Trainer Class\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForTokenClassification(tokenizer),\n    callbacks=[early_stopping_callback],  # Added early stopping\n)\n\n# ✅ Training Loop with Progress Bar & Metrics\nprint(\"🚀 Starting Training...\")\nstart_time = time.time()\n\nfor epoch in range(training_args.num_train_epochs):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n\n    for batch in progress_bar:\n        optimizer.zero_grad()  # ✅ Clear previous gradients\n        outputs = model(**batch)\n        loss = outputs[0]  # Unpack loss from the tuple\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = torch.argmax(outputs[1], dim=2)  # Get logits from the second element of the tuple\n\n        # ✅ Calculate Accuracy and F1 Score\n        for i in range(len(batch[\"labels\"])):\n            true_labels = batch[\"labels\"][i].cpu().numpy()\n            pred_labels = preds[i].cpu().numpy()\n            mask = true_labels != -100  # ✅ Ignore padding tokens\n            correct += (true_labels[mask] == pred_labels[mask]).sum()\n            total += mask.sum()\n\n        avg_loss = total_loss / len(train_dataloader)\n        accuracy = correct / total if total > 0 else 0\n\n        # Compute F1 Score after each epoch\n        f1 = f1_score(true_labels, pred_labels, average=\"weighted\")  # F1 score for each batch\n\n        progress_bar.set_postfix({\"Loss\": avg_loss, \"Accuracy\": accuracy, \"F1\": f1})\n\n# ✅ Save Model\nmodel.save_pretrained(\"xlm-roberta-large\")  # Save the fine-tuned model\ntokenizer.save_pretrained(\"xlm-roberta-large\")\n\nend_time = time.time()\nprint(f\"✅ Training Completed in {round(end_time - start_time, 2)} seconds\")\nprint(f\"✅ Final Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:37:44.620628Z","iopub.execute_input":"2025-03-26T10:37:44.620938Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-15-d7e02277fda7>:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"🚀 Starting Training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 765/765 [06:11<00:00,  2.06it/s, Loss=0.609, Accuracy=0.741, F1=0.291]   \nEpoch 2: 100%|██████████| 765/765 [06:11<00:00,  2.06it/s, Loss=0.624, Accuracy=0.739, F1=0.873]   \nEpoch 3: 100%|██████████| 765/765 [06:10<00:00,  2.06it/s, Loss=0.624, Accuracy=0.739, F1=0.000121]\nEpoch 4: 100%|██████████| 765/765 [06:10<00:00,  2.06it/s, Loss=0.623, Accuracy=0.739, F1=0.884]   \nEpoch 5:  64%|██████▍   | 492/765 [03:58<02:12,  2.06it/s, Loss=0.398, Accuracy=0.743, F1=0.000121]","output_type":"stream"}],"execution_count":null}]}