{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:55:03.819398Z","iopub.execute_input":"2025-03-31T13:55:03.819736Z","iopub.status.idle":"2025-03-31T13:55:04.128642Z","shell.execute_reply.started":"2025-03-31T13:55:03.819703Z","shell.execute_reply":"2025-03-31T13:55:04.127991Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/unlp-2025-shared-task-span-identification/sample_submission.csv\n/kaggle/input/unlp-2025-shared-task-span-identification/README.md\n/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet\n/kaggle/input/unlp-2025-shared-task-span-identification/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_parquet('/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet')\ntest_df = pd.read_csv('/kaggle/input/unlp-2025-shared-task-span-identification/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/unlp-2025-shared-task-span-identification/sample_submission.csv')\n\n# Display the first few rows of each dataset to inspect their structure\nprint(\"Train DataFrame:\")\nprint(train_df.head())\n\nprint(\"\\nTest DataFrame:\")\nprint(test_df.head())\n\nprint(\"\\nSample Submission DataFrame:\")\nprint(sample_submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:55:04.129656Z","iopub.execute_input":"2025-03-31T13:55:04.130010Z","iopub.status.idle":"2025-03-31T13:55:04.653772Z","shell.execute_reply.started":"2025-03-31T13:55:04.129990Z","shell.execute_reply":"2025-03-31T13:55:04.652997Z"}},"outputs":[{"name":"stdout","text":"Train DataFrame:\n                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content lang  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   uk          True   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   uk         False   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n2        [loaded_language, euphoria]   \n3                               None   \n4                  [loaded_language]   \n\n                                   trigger_words  \n0    [[27, 63], [65, 88], [90, 183], [186, 308]]  \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]  \n2                                    [[55, 100]]  \n3                                           None  \n4                                   [[114, 144]]  \n\nTest DataFrame:\n                                     id  \\\n0  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba   \n1  9b2a61e4-d14e-4ff7-b304-e73d720319bf   \n2  f0f1c236-80a8-4d25-b30c-a420a39be632   \n3  31ea05ba-2c2b-4b84-aba7-f3cf6841b204   \n4  a79e13ec-6d9a-40b5-b54c-7f4f743a7525   \n\n                                             content  \n0  –û–Ω–∏ –ø—Ä–æ—Å—Ä–∞–ª–∏ –Ω–∞—à—É —Ç–µ—Ö–Ω–∏–∫—É, –ø–æ–ª–æ–∂–∏–ª–∏ –∫—É—á—É –ª—é–¥–µ–π...  \n1  ‚ùóÔ∏è\\n–ö–∏—Ç–∞–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ—Ç–¥–∞—Ç—å –æ–∫–∫—É–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä...  \n2  –°–µ–≥–æ–¥–Ω—è –±—É–¥–µ—Ç —Ä–æ–≤–Ω–æ 6 –º–µ—Å—è—Ü–µ–≤ —Å —ç—Ç–æ–≥–æ –æ–±–µ—â–∞–Ω–∏—è...  \n3  ‚ö°Ô∏è\\n–Ü–∑—Ä–∞—ó–ª—å –≤–ø–µ—Ä—à–µ —É —Å–≤—ñ—Ç—ñ –∑–±–∏–≤ –±–∞–ª—ñ—Å—Ç–∏—á–Ω—É —Ä–∞–∫...  \n4  –°–∫–ª–∞–≤ –Ω–µ–≤–µ–ª–∏–∫—É –Ω–∞–≤—á–∞–ª—å–Ω–æ-–º–µ—Ç–æ–¥–∏—á–Ω—É —Ç–∞–±–ª–∏—Ü—é –Ω–∞ ...  \n\nSample Submission DataFrame:\n                                     id trigger_words\n0  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba      [(0, 1)]\n1  9b2a61e4-d14e-4ff7-b304-e73d720319bf      [(0, 1)]\n2  f0f1c236-80a8-4d25-b30c-a420a39be632      [(0, 1)]\n3  31ea05ba-2c2b-4b84-aba7-f3cf6841b204      [(0, 1)]\n4  a79e13ec-6d9a-40b5-b54c-7f4f743a7525      [(0, 1)]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hfsb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:55:04.654632Z","iopub.execute_input":"2025-03-31T13:55:04.654933Z","iopub.status.idle":"2025-03-31T13:55:04.759313Z","shell.execute_reply.started":"2025-03-31T13:55:04.654898Z","shell.execute_reply":"2025-03-31T13:55:04.758720Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Step 1: Handle missing values in 'trigger_words' and 'techniques'\ntrain_df['trigger_words'] = train_df['trigger_words'].apply(lambda x: eval(x) if isinstance(x, str) else [])\ntrain_df['techniques'] = train_df['techniques'].apply(lambda x: eval(x) if isinstance(x, str) else [])\n\n# Step 2: Tokenization (using HuggingFace's tokenizer)\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# Replace '<your_token>' with the retrieved token from Kaggle Secrets\ntokenizer = AutoTokenizer.from_pretrained('SpanBERT/spanbert-large-cased', use_auth_token=secret_value_0)\nmodel = AutoModelForTokenClassification.from_pretrained('SpanBERT/spanbert-large-cased', use_auth_token=secret_value_0)\n\n\n# Function to tokenize and handle truncation\ndef tokenize_with_truncation(text, max_length=512):\n    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length)\n    return tokens\n\n# Tokenize the 'content' column with truncation\ntrain_df['tokens'] = train_df['content'].apply(lambda x: tokenize_with_truncation(x))\n\n# Step 3: Convert 'manipulative' target to numeric values\ntrain_df['manipulative'] = train_df['manipulative'].astype(int)\n\n# Step 4: Ensure the 'trigger_words' spans are correctly formatted for token-based indices\ndef get_tokenized_trigger_words(text, trigger_words, tokenizer, max_length=512):\n    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length)\n    trigger_word_indices = []\n    for span in trigger_words:\n        start, end = span\n        # Adjust start and end positions based on truncation\n        start_token_index = len(tokenizer.encode(text[:start], add_special_tokens=False, truncation=True, max_length=max_length))\n        end_token_index = len(tokenizer.encode(text[:end], add_special_tokens=False, truncation=True, max_length=max_length))\n        trigger_word_indices.append([start_token_index, end_token_index])\n    return trigger_word_indices\n\ntrain_df['tokenized_trigger_words'] = train_df.apply(\n    lambda row: get_tokenized_trigger_words(row['content'], row['trigger_words'], tokenizer),\n    axis=1\n)\n\n# Check the processed data\nprint(train_df[['content', 'tokens', 'tokenized_trigger_words', 'manipulative']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:55:04.760075Z","iopub.execute_input":"2025-03-31T13:55:04.760325Z","iopub.status.idle":"2025-03-31T13:55:39.257330Z","shell.execute_reply.started":"2025-03-31T13:55:04.760304Z","shell.execute_reply":"2025-03-31T13:55:39.256440Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744d299566664156b61ac8eced3a67df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf535041efb422f91a7decf4d9e24dd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/665M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000eb4a0d1174add98655e3b0cb07dfd"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"                                             content  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   \n\n                                              tokens tokenized_trigger_words  \\\n0  [101, 488, 24625, 17424, 17424, 489, 28395, 28...                      []   \n1  [101, 488, 19692, 28396, 10286, 28394, 17127, ...                      []   \n2  [101, 100, 493, 17424, 28401, 498, 10286, 2840...                      []   \n3  [101, 477, 494, 28399, 20442, 10286, 28418, 17...                      []   \n4  [101, 491, 10286, 28403, 28409, 19692, 28404, ...                      []   \n\n   manipulative  \n0             1  \n1             1  \n2             1  \n3             0  \n4             1  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Check if there are any empty trigger words\nempty_trigger_count = train_df[train_df['tokenized_trigger_words'].apply(lambda x: len(x) == 0)].shape[0]\nprint(f\"Number of rows with empty trigger words: {empty_trigger_count}\")\n\n# Handle rows with no trigger words (optionally, we could mark them as 'None' or fill them)\ntrain_df['tokenized_trigger_words'] = train_df['tokenized_trigger_words'].apply(lambda x: x if len(x) > 0 else None)\n\n# Check the result again\nprint(train_df[['content', 'tokenized_trigger_words']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:55:39.259054Z","iopub.execute_input":"2025-03-31T13:55:39.259584Z","iopub.status.idle":"2025-03-31T13:55:39.271604Z","shell.execute_reply.started":"2025-03-31T13:55:39.259556Z","shell.execute_reply":"2025-03-31T13:55:39.270721Z"}},"outputs":[{"name":"stdout","text":"Number of rows with empty trigger words: 3822\n                                             content tokenized_trigger_words\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...                    None\n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...                    None\n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...                    None\n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...                    None\n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...                    None\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# Replace '<your_token>' with the retrieved token from Kaggle Secrets\ntokenizer = AutoTokenizer.from_pretrained('SpanBERT/spanbert-large-cased', use_auth_token=secret_value_0)\nmodel = AutoModelForTokenClassification.from_pretrained('SpanBERT/spanbert-large-cased', use_auth_token=secret_value_0)\n\n\n# Tokenize the 'content' column\ntrain_df['tokenized_content'] = train_df['content'].apply(lambda x: tokenizer.encode(x, truncation=True, padding='max_length', max_length=512))\n\n# Check the result\nprint(train_df[['content', 'tokenized_content']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:56:28.157418Z","iopub.execute_input":"2025-03-31T13:56:28.157772Z","iopub.status.idle":"2025-03-31T13:56:33.949180Z","shell.execute_reply.started":"2025-03-31T13:56:28.157748Z","shell.execute_reply":"2025-03-31T13:56:33.948259Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"                                             content  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   \n\n                                   tokenized_content  \n0  [101, 488, 24625, 17424, 17424, 489, 28395, 28...  \n1  [101, 488, 19692, 28396, 10286, 28394, 17127, ...  \n2  [101, 100, 493, 17424, 28401, 498, 10286, 2840...  \n3  [101, 477, 494, 28399, 20442, 10286, 28418, 17...  \n4  [101, 491, 10286, 28403, 28409, 19692, 28404, ...  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import ast\n\n# Step 1: Map 'manipulative' column to binary labels (True -> 1, False -> 0)\ntrain_df['manipulative_label'] = train_df['manipulative'].map({True: 1, False: 0})\n\n# Save the dataframe with 'manipulative_label' as a CSV file\ntrain_df[['id', 'content', 'manipulative', 'manipulative_label']].to_csv('/kaggle/working/train_with_labels.csv', index=False)\n\n# Step 2: Convert 'trigger_words' from string to actual list of tuples (start, end)\n# Apply the conversion only when the value is not None or 'no_trigger'\ntrain_df['trigger_words'] = train_df['tokenized_trigger_words'].apply(\n    lambda x: ast.literal_eval(x) if x not in [None, 'no_trigger'] else []\n)\n\n# Save the dataframe with formatted 'trigger_words' as a CSV file\ntrain_df[['id', 'content', 'tokenized_trigger_words', 'trigger_words']].to_csv('/kaggle/working/train_with_trigger_words.csv', index=False)\n\n# Output the head of the dataframe to confirm the changes\nprint(train_df[['id', 'content', 'manipulative', 'manipulative_label', 'trigger_words']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:56:37.167007Z","iopub.execute_input":"2025-03-31T13:56:37.167312Z","iopub.status.idle":"2025-03-31T13:56:37.334388Z","shell.execute_reply.started":"2025-03-31T13:56:37.167288Z","shell.execute_reply":"2025-03-31T13:56:37.333326Z"}},"outputs":[{"name":"stdout","text":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...             1   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...             1   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...             1   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...             0   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...             1   \n\n   manipulative_label trigger_words  \n0                 NaN            []  \n1                 NaN            []  \n2                 NaN            []  \n3                 NaN            []  \n4                 NaN            []  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\n# Load the datasets\ntrain_with_labels_df = pd.read_csv('/kaggle/working/train_with_labels.csv')\ntrain_with_trigger_words_df = pd.read_csv('/kaggle/working/train_with_trigger_words.csv')\n\n# Display the first few rows of the datasets\ntrain_with_labels_df.head(), train_with_trigger_words_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:56:40.577095Z","iopub.execute_input":"2025-03-31T13:56:40.577393Z","iopub.status.idle":"2025-03-31T13:56:40.739743Z","shell.execute_reply.started":"2025-03-31T13:56:40.577371Z","shell.execute_reply":"2025-03-31T13:56:40.738828Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(                                     id  \\\n 0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n 1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n 2  e6a427f1-211f-405f-bd8b-70798458d656   \n 3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n 4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n \n                                              content  manipulative  \\\n 0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...             1   \n 1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...             1   \n 2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...             1   \n 3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...             0   \n 4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...             1   \n \n    manipulative_label  \n 0                 NaN  \n 1                 NaN  \n 2                 NaN  \n 3                 NaN  \n 4                 NaN  ,\n                                      id  \\\n 0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n 1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n 2  e6a427f1-211f-405f-bd8b-70798458d656   \n 3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n 4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n \n                                              content  tokenized_trigger_words  \\\n 0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...                      NaN   \n 1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...                      NaN   \n 2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...                      NaN   \n 3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...                      NaN   \n 4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...                      NaN   \n \n   trigger_words  \n 0            []  \n 1            []  \n 2            []  \n 3            []  \n 4            []  )"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import ast\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n\n\n# Step 1: Populate manipulative_label\ntrain_with_labels_df['manipulative_label'] = train_with_labels_df['manipulative']\n\n# Step 2: Handle missing trigger words\ntrain_with_trigger_words_df['trigger_words'] = train_with_trigger_words_df['trigger_words'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x != 'no_trigger' else [])\n\n# Tokenize the content\n# Replace '<your_token>' with the retrieved token from Kaggle Secrets\ntokenizer = AutoTokenizer.from_pretrained('SpanBERT/spanbert-large-cased', use_auth_token=secret_value_0)\n\n\n# Function to tokenize and pad sequences\ndef tokenize_and_pad(text):\n    return tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n\n# Apply tokenization to the content column\ntrain_with_trigger_words_df['tokenized_content'] = train_with_trigger_words_df['content'].apply(lambda x: tokenize_and_pad(x))\n\n# Save the updated DataFrame with tokenized content\ntrain_with_trigger_words_df.to_csv('/kaggle/working/train_with_tokenized_content.csv', index=False)\n\n# Check the updated DataFrame\ntrain_with_trigger_words_df[['id', 'content', 'tokenized_content']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:56:43.854309Z","iopub.execute_input":"2025-03-31T13:56:43.854663Z","iopub.status.idle":"2025-03-31T13:57:06.773175Z","shell.execute_reply.started":"2025-03-31T13:56:43.854636Z","shell.execute_reply":"2025-03-31T13:57:06.772314Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   \n\n                             tokenized_content  \n0  [input_ids, token_type_ids, attention_mask]  \n1  [input_ids, token_type_ids, attention_mask]  \n2  [input_ids, token_type_ids, attention_mask]  \n3  [input_ids, token_type_ids, attention_mask]  \n4  [input_ids, token_type_ids, attention_mask]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>tokenized_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bb0c7fa-101b-4583-a5f9-9d503339141c</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7159f802-6f99-4e9d-97bd-6f565a4a0fae</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e6a427f1-211f-405f-bd8b-70798458d656</td>\n      <td>ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1647a352-4cd3-40f6-bfa1-d87d42e34eea</td>\n      <td>–í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c01de00-841f-4b50-9407-104e9ffb03bf</td>\n      <td>–†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n\n\n# Assuming you have already loaded the dataset in train_with_trigger_words_df and train_with_labels_df\ntrain_with_trigger_words_df = pd.read_csv('/kaggle/working/train_with_trigger_words.csv')\ntrain_with_labels_df = pd.read_csv('/kaggle/working/train_with_labels.csv')\n\n# Step 1: Check if the 'manipulative' column exists\nif 'manipulative' not in train_with_trigger_words_df.columns:\n    print(\"Error: 'manipulative' column is missing in the dataset.\")\n    # If missing, manually add a dummy or placeholder 'manipulative' column for further processing:\n    # You may need to replace this with your actual logic if you have another source for this column.\n    # For now, assuming the 'manipulative' column should be derived from some other logic.\n    \n    # Example of creating a dummy column based on index\n    train_with_trigger_words_df['manipulative'] = [True if i % 2 == 0 else False for i in range(len(train_with_trigger_words_df))]\n    print(\"Added dummy 'manipulative' column for testing.\")\n\n# Step 2: Map 'manipulative' column to binary labels (True -> 1, False -> 0)\ntrain_with_trigger_words_df['manipulative_label'] = train_with_trigger_words_df['manipulative'].map({True: 1, False: 0})\n\n# Step 3: Handle missing 'trigger_words'\ntrain_with_trigger_words_df['trigger_words'] = train_with_trigger_words_df['trigger_words'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x != 'no_trigger' else [])\n\n# Step 4: Tokenization using HuggingFace's BERT tokenizer\n# Replace '<your_token>' with the retrieved token from Kaggle Secrets\ntokenizer = AutoTokenizer.from_pretrained('SpanBERT/spanbert-large-cased', use_auth_token=secret_value_0)\n\n\ndef tokenize_and_pad(text):\n    encoding = tokenizer.encode_plus(\n        text, \n        padding='max_length', \n        truncation=True, \n        max_length=512, \n        return_tensors='pt'\n    )\n    # Extract token_ids from the encoding\n    return encoding['input_ids'].squeeze().tolist()  # Convert tensor to list\n\n# Apply tokenization\ntrain_with_trigger_words_df['tokenized_content'] = train_with_trigger_words_df['content'].apply(lambda x: tokenize_and_pad(x))\n\n# Save the updated DataFrame with tokenized content\ntrain_with_trigger_words_df.to_csv('/kaggle/working/train_with_tokenized_content.csv', index=False)\n\n# Check the updated DataFrame\nprint(train_with_trigger_words_df[['id', 'content', 'tokenized_content']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:57:15.869117Z","iopub.execute_input":"2025-03-31T13:57:15.869434Z","iopub.status.idle":"2025-03-31T13:57:21.572444Z","shell.execute_reply.started":"2025-03-31T13:57:15.869406Z","shell.execute_reply":"2025-03-31T13:57:21.571637Z"}},"outputs":[{"name":"stdout","text":"Error: 'manipulative' column is missing in the dataset.\nAdded dummy 'manipulative' column for testing.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   \n\n                                   tokenized_content  \n0  [101, 488, 24625, 17424, 17424, 489, 28395, 28...  \n1  [101, 488, 19692, 28396, 10286, 28394, 17127, ...  \n2  [101, 100, 493, 17424, 28401, 498, 10286, 2840...  \n3  [101, 477, 494, 28399, 20442, 10286, 28418, 17...  \n4  [101, 491, 10286, 28403, 28409, 19692, 28404, ...  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForTokenClassification\n# import torch\n\n# # Define the device (GPU if available, otherwise CPU)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# hf_token = \"hfsb\"  # Replace with your actual token\n\n# # Load the tokenizer and model with token authentication\n# tokenizer = AutoTokenizer.from_pretrained('microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank')\n\n# # Use ignore_mismatched_sizes=True to avoid errors for mismatched layer sizes\n# model = AutoModelForTokenClassification.from_pretrained(\n#     'microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank',\n#     num_labels=2,  # Set the number of labels according to your task\n#     use_auth_token=hf_token,\n#     ignore_mismatched_sizes=True  # This allows for size mismatch in the classifier layer\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:33:07.188376Z","iopub.execute_input":"2025-03-31T15:33:07.188626Z","iopub.status.idle":"2025-03-31T15:33:07.192339Z","shell.execute_reply.started":"2025-03-31T15:33:07.188606Z","shell.execute_reply":"2025-03-31T15:33:07.191445Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import torch\nimport time\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom huggingface_hub import login\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\nfrom torch.optim import Adam\nimport pandas as pd\n\n# Step 1: Authenticate with Hugging Face\n# Replace 'your_huggingface_token' with your actual Hugging Face API token\nlogin(token=\"roberta\")\n\n# Step 2: Prepare the model and tokenizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"Nic1/roberta-finetuned-propaganda-span-identification\"\n\n# Load the tokenizer and model from Hugging Face Model Hub using authentication\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name, use_auth_token=True)\nmodel.to(device)\n\n# Step 3: Prepare the Dataset\nclass TokenClassificationDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.tokenizer = tokenizer\n        self.data = df\n        self.max_length = 512  # Max length for XLM-RoBERTa\n\n    def __getitem__(self, idx):\n        content = self.data.iloc[idx]['content']\n        label = self.data.iloc[idx]['manipulative_label']\n        \n        # Tokenize the content with padding and truncation\n        encoding = self.tokenizer(content, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n        \n        # Get input_ids and attention_mask from encoding\n        input_ids = encoding['input_ids'].squeeze(0).to(device)\n        attention_mask = encoding['attention_mask'].squeeze(0).to(device)\n\n        # Create labels tensor with padding (should be the same length as input_ids)\n        labels_tensor = torch.full_like(input_ids, -100).to(device)  # Use -100 to ignore padding tokens during loss computation\n        \n        # Set the label for the entire sequence (all tokens)\n        labels_tensor[:] = label  # Assign the same label to all tokens in the sequence\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels_tensor,\n        }\n\n    def __len__(self):\n        return len(self.data)\n\n# Load your dataset (replace this with your actual dataframe loading method)\ntrain_with_tokenized_content_df = pd.read_csv(\"/kaggle/working/train_with_tokenized_content.csv\")\n\n# Create dataset for training\ntrain_dataset = TokenClassificationDataset(train_with_tokenized_content_df)\n\n# Step 4: Training Arguments\ntraining_args = {\n    'output_dir': '/kaggle/working',          # Directory to save model\n    'num_train_epochs': 5,                   # Number of epochs\n    'per_device_train_batch_size': 12,        # Batch size per device\n    'per_device_eval_batch_size': 12,         # Batch size for evaluation\n    'warmup_steps': 500,                     # Warm-up steps\n    'weight_decay': 0.01,                    # Strength of weight decay\n    'logging_dir': '/kaggle/working/logs',    # Directory for logs\n    'logging_steps': 10,\n    'evaluation_strategy': \"epoch\",          # Evaluate after each epoch\n    'save_strategy': \"epoch\",                # Save the model after each epoch\n    'load_best_model_at_end': True,          # Load the best model at the end\n    'report_to': \"none\",                     # Disable reporting to Huggingface Hub\n    'disable_tqdm': False,                   # Enable progress bars\n    'dataloader_num_workers': 2,             # Number of workers for the data loader\n}\n\n# Step 5: Training loop and optimizer setup\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\ndef train_model(model, train_dataset, epochs=5):\n    for epoch in range(epochs):\n        start_time = time.time()\n        model.train()\n        \n        # Create data loader for batching\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n        total_loss = 0\n        correct_predictions = 0\n        total_predictions = 0\n        \n        # Loop through the batches and perform training\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):\n            optimizer.zero_grad()  # Reset gradients\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n\n            # Forward pass\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            predictions = torch.argmax(logits, dim=-1)\n            correct_predictions += (predictions == labels).sum().item()\n            total_predictions += labels.numel()\n\n        # Calculate average loss and accuracy for the epoch\n        avg_loss = total_loss / len(train_loader)\n        accuracy = correct_predictions / total_predictions\n\n        # Compute F1 score (macro)\n        f1 = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n\n        # Log the metrics\n        epoch_duration = time.time() - start_time\n        print(f'Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f} | F1 Score: {f1:.4f} | Time: {epoch_duration:.2f}s')\n\n# Step 6: Train the model\ntrain_model(model, train_dataset, epochs=5)\n\n# Save the model's state_dict\noutput_dir = \"/kaggle/working/xlm-roberta-model\"\nos.makedirs(output_dir, exist_ok=True)\ntorch.save(model.state_dict(), f\"{output_dir}/model_state_dict.pth\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(output_dir)\n\nprint(\"Training completed and model saved!\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:48:47.188390Z","iopub.execute_input":"2025-03-31T15:48:47.188747Z","iopub.status.idle":"2025-03-31T16:25:28.861975Z","shell.execute_reply.started":"2025-03-31T15:48:47.188719Z","shell.execute_reply":"2025-03-31T16:25:28.861205Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nEpoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [07:20<00:00,  1.08batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 | Loss: 0.7125 | Accuracy: 0.5003 | F1 Score: 0.0000 | Time: 440.97s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [07:19<00:00,  1.09batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 | Loss: 0.6949 | Accuracy: 0.5095 | F1 Score: 0.4855 | Time: 439.73s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [07:19<00:00,  1.09batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 | Loss: 0.6954 | Accuracy: 0.4947 | F1 Score: 0.8177 | Time: 439.86s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [07:19<00:00,  1.09batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 | Loss: 0.6948 | Accuracy: 0.5057 | F1 Score: 0.7990 | Time: 439.64s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [07:19<00:00,  1.09batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 | Loss: 0.6948 | Accuracy: 0.4951 | F1 Score: 0.5451 | Time: 439.57s\nTraining completed and model saved!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import torch   \nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Step 6: Model Evaluation on Test Set\n\n# Load test data\ntest_df = pd.read_csv('/kaggle/input/unlp-2025-shared-task-span-identification/test.csv')\n\n# Tokenize the test data\ntest_encodings = tokenizer(list(test_df['content']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n\n# Prepare the labels (if ground truth is available)\n# For inference purposes, we'll predict without labels\n# Placeholder labels (for token classification, each token has a label, not just a sequence)\nlabels = [[0] * len(test_encodings['input_ids'][i]) for i in range(len(test_df))]  # Placeholder, no ground truth\n\n# Create a custom dataset for the test set (same structure as the training dataset)\nclass ManipulationDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx]).to(device)  # Label per token\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create the test dataset\ntest_dataset = ManipulationDataset(test_encodings, labels)\n\n# Create DataLoader for batching during evaluation\ntest_dataloader = DataLoader(test_dataset, batch_size=8)\n\n# Put model in evaluation mode\nmodel.eval()\n\nall_preds = []\n\n# Disable gradient calculation for inference\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, desc=\"Evaluating on Test Set\"):\n        # Move batch to GPU\n        batch = {key: val.to(device) for key, val in batch.items()}\n\n        # Forward pass\n        outputs = model(input_ids=batch['input_ids'],\n                        attention_mask=batch['attention_mask'],\n                        labels=batch['labels'])  # Correctly handle labels for token classification\n\n        logits = outputs.logits\n        \n        # Get predictions\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        all_preds.extend(preds)\n\n# Prepare the final submission format with 'trigger_words' column\ndef get_trigger_words(predictions):\n    trigger_words = []\n    for pred in predictions:\n        spans = []\n        for idx, token in enumerate(pred):\n            if token == 1:  # If the token is predicted as a trigger (1)\n                start = idx\n                end = idx + 1\n                spans.append((start, end))\n        trigger_words.append(spans if spans else \"[]\")\n    return trigger_words\n\n# Generate 'trigger_words' for each test sample\ntrigger_words = get_trigger_words(all_preds)\n\n# Add the 'trigger_words' to the test dataframe\ntest_df['trigger_words'] = trigger_words\n\n# Save the result in the required format\nsubmission_df = test_df[['id', 'trigger_words']]\n\n# Save the predictions to a CSV file in the submission format\nsubmission_df.to_csv('/kaggle/working/final_submission.csv', index=False)\n\nprint(\"Predictions saved to /kaggle/working/final_submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:25:44.819932Z","iopub.execute_input":"2025-03-31T16:25:44.820226Z","iopub.status.idle":"2025-03-31T16:28:48.006268Z","shell.execute_reply.started":"2025-03-31T16:25:44.820205Z","shell.execute_reply":"2025-03-31T16:28:48.005365Z"}},"outputs":[{"name":"stderr","text":"Evaluating on Test Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 717/717 [02:57<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/final_submission.csv\n","output_type":"stream"}],"execution_count":28}]}